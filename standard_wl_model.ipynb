{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ead66db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, Concatenate, Lambda, Conv1D, BatchNormalization\n",
    "\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd4885c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load validation data\n",
    "save_path = \"data/validation_data/\"\n",
    "X_train = np.load(save_path + \"X_train.npy\")\n",
    "Y_train = np.load(save_path + \"Y_train.npy\")#[:, 0]\n",
    "X_test = np.load(save_path + \"X_test.npy\")\n",
    "Y_test = np.load(save_path + \"Y_test.npy\")#[:, 0]\n",
    "X_val = np.load(save_path + \"X_val.npy\")\n",
    "Y_val = np.load(save_path + \"Y_val.npy\")#[:, 0]\n",
    "\n",
    "# Only use win-loss target for training (wl models don't need odds)\n",
    "Y_train = Y_train[:, 0]\n",
    "Y_test = Y_test[:, 0]\n",
    "Y_val = Y_val[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e42457e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "700/700 [==============================] - 3s 3ms/step - loss: 1.2782 - accuracy: 0.5301 - val_loss: 0.9421 - val_accuracy: 0.5688\n",
      "Epoch 2/200\n",
      "700/700 [==============================] - 2s 3ms/step - loss: 0.7985 - accuracy: 0.5655 - val_loss: 0.7195 - val_accuracy: 0.5600\n",
      "Epoch 3/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.7051 - accuracy: 0.5555 - val_loss: 0.6997 - val_accuracy: 0.5379\n",
      "Epoch 4/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6953 - accuracy: 0.5495 - val_loss: 0.6952 - val_accuracy: 0.5383\n",
      "Epoch 5/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6913 - accuracy: 0.5495 - val_loss: 0.6915 - val_accuracy: 0.5383\n",
      "Epoch 6/200\n",
      "700/700 [==============================] - 2s 3ms/step - loss: 0.6886 - accuracy: 0.5495 - val_loss: 0.6908 - val_accuracy: 0.5383\n",
      "Epoch 7/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6886 - accuracy: 0.5495 - val_loss: 0.6908 - val_accuracy: 0.5383\n",
      "Epoch 8/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6886 - accuracy: 0.5495 - val_loss: 0.6908 - val_accuracy: 0.5383\n",
      "Epoch 9/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6886 - accuracy: 0.5495 - val_loss: 0.6907 - val_accuracy: 0.5383\n",
      "Epoch 10/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6886 - accuracy: 0.5495 - val_loss: 0.6907 - val_accuracy: 0.5383\n",
      "Epoch 11/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6886 - accuracy: 0.5495 - val_loss: 0.6908 - val_accuracy: 0.5383\n",
      "Epoch 12/200\n",
      "700/700 [==============================] - 2s 3ms/step - loss: 0.6886 - accuracy: 0.5495 - val_loss: 0.6907 - val_accuracy: 0.5383\n",
      "Epoch 13/200\n",
      "700/700 [==============================] - 2s 3ms/step - loss: 0.6886 - accuracy: 0.5495 - val_loss: 0.6908 - val_accuracy: 0.5383\n",
      "Epoch 14/200\n",
      "700/700 [==============================] - 2s 3ms/step - loss: 0.6886 - accuracy: 0.5495 - val_loss: 0.6908 - val_accuracy: 0.5383\n",
      "Epoch 15/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6886 - accuracy: 0.5495 - val_loss: 0.6908 - val_accuracy: 0.5383\n",
      "Epoch 16/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6886 - accuracy: 0.5495 - val_loss: 0.6907 - val_accuracy: 0.5383\n",
      "Epoch 16: early stopping\n",
      "Validation Accuracy: 53.83%\n",
      "Skipped.\n",
      "Epoch 1/200\n",
      "700/700 [==============================] - 2s 3ms/step - loss: 1.2620 - accuracy: 0.5408 - val_loss: 0.9367 - val_accuracy: 0.5587\n",
      "Epoch 2/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.7993 - accuracy: 0.5530 - val_loss: 0.7348 - val_accuracy: 0.5383\n",
      "Epoch 3/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.7122 - accuracy: 0.5495 - val_loss: 0.6997 - val_accuracy: 0.5383\n",
      "Epoch 4/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6921 - accuracy: 0.5495 - val_loss: 0.6912 - val_accuracy: 0.5383\n",
      "Epoch 5/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6887 - accuracy: 0.5495 - val_loss: 0.6907 - val_accuracy: 0.5383\n",
      "Epoch 6/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6886 - accuracy: 0.5495 - val_loss: 0.6907 - val_accuracy: 0.5383\n",
      "Epoch 7/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6886 - accuracy: 0.5495 - val_loss: 0.6907 - val_accuracy: 0.5383\n",
      "Epoch 8/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6886 - accuracy: 0.5495 - val_loss: 0.6909 - val_accuracy: 0.5383\n",
      "Epoch 9/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6886 - accuracy: 0.5495 - val_loss: 0.6907 - val_accuracy: 0.5383\n",
      "Epoch 10/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6886 - accuracy: 0.5495 - val_loss: 0.6906 - val_accuracy: 0.5383\n",
      "Epoch 11/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6886 - accuracy: 0.5495 - val_loss: 0.6907 - val_accuracy: 0.5383\n",
      "Epoch 12/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6886 - accuracy: 0.5495 - val_loss: 0.6907 - val_accuracy: 0.5383\n",
      "Epoch 13/200\n",
      "700/700 [==============================] - 2s 3ms/step - loss: 0.6886 - accuracy: 0.5495 - val_loss: 0.6908 - val_accuracy: 0.5383\n",
      "Epoch 14/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6886 - accuracy: 0.5495 - val_loss: 0.6908 - val_accuracy: 0.5383\n",
      "Epoch 15/200\n",
      "700/700 [==============================] - 2s 3ms/step - loss: 0.6886 - accuracy: 0.5495 - val_loss: 0.6907 - val_accuracy: 0.5383\n",
      "Epoch 16/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6886 - accuracy: 0.5495 - val_loss: 0.6907 - val_accuracy: 0.5383\n",
      "Epoch 17/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6886 - accuracy: 0.5495 - val_loss: 0.6907 - val_accuracy: 0.5383\n",
      "Epoch 17: early stopping\n",
      "Validation Accuracy: 53.83%\n",
      "Skipped.\n",
      "Epoch 1/200\n",
      "700/700 [==============================] - 2s 3ms/step - loss: 1.3909 - accuracy: 0.5294 - val_loss: 1.0838 - val_accuracy: 0.5612\n",
      "Epoch 2/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.9015 - accuracy: 0.5678 - val_loss: 0.7772 - val_accuracy: 0.5567\n",
      "Epoch 3/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.7365 - accuracy: 0.5651 - val_loss: 0.7187 - val_accuracy: 0.5517\n",
      "Epoch 4/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.7079 - accuracy: 0.5538 - val_loss: 0.7015 - val_accuracy: 0.5546\n",
      "Epoch 5/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6966 - accuracy: 0.5508 - val_loss: 0.6953 - val_accuracy: 0.5387\n",
      "Epoch 6/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6919 - accuracy: 0.5496 - val_loss: 0.6925 - val_accuracy: 0.5383\n",
      "Epoch 7/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6905 - accuracy: 0.5495 - val_loss: 0.6922 - val_accuracy: 0.5383\n",
      "Epoch 8/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6890 - accuracy: 0.5495 - val_loss: 0.6907 - val_accuracy: 0.5383\n",
      "Epoch 9/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6886 - accuracy: 0.5495 - val_loss: 0.6907 - val_accuracy: 0.5383\n",
      "Epoch 10/200\n",
      "700/700 [==============================] - 2s 3ms/step - loss: 0.6886 - accuracy: 0.5495 - val_loss: 0.6907 - val_accuracy: 0.5383\n",
      "Epoch 11/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6886 - accuracy: 0.5495 - val_loss: 0.6907 - val_accuracy: 0.5383\n",
      "Epoch 12/200\n",
      "700/700 [==============================] - 2s 3ms/step - loss: 0.6886 - accuracy: 0.5495 - val_loss: 0.6907 - val_accuracy: 0.5383\n",
      "Epoch 13/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6886 - accuracy: 0.5495 - val_loss: 0.6908 - val_accuracy: 0.5383\n",
      "Epoch 14/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6886 - accuracy: 0.5495 - val_loss: 0.6908 - val_accuracy: 0.5383\n",
      "Epoch 15/200\n",
      "700/700 [==============================] - 2s 3ms/step - loss: 0.6886 - accuracy: 0.5495 - val_loss: 0.6907 - val_accuracy: 0.5383\n",
      "Epoch 16/200\n",
      "700/700 [==============================] - 2s 3ms/step - loss: 0.6887 - accuracy: 0.5495 - val_loss: 0.6907 - val_accuracy: 0.5383\n",
      "Epoch 16: early stopping\n",
      "Validation Accuracy: 53.83%\n",
      "Skipped.\n",
      "Epoch 1/200\n",
      "700/700 [==============================] - 2s 3ms/step - loss: 1.3639 - accuracy: 0.5194 - val_loss: 1.0495 - val_accuracy: 0.5554\n",
      "Epoch 2/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.8981 - accuracy: 0.5580 - val_loss: 0.7912 - val_accuracy: 0.5721\n",
      "Epoch 3/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.7432 - accuracy: 0.5520 - val_loss: 0.7160 - val_accuracy: 0.5383\n",
      "Epoch 4/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.7053 - accuracy: 0.5495 - val_loss: 0.7000 - val_accuracy: 0.5383\n",
      "Epoch 5/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6948 - accuracy: 0.5495 - val_loss: 0.6939 - val_accuracy: 0.5383\n",
      "Epoch 6/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6908 - accuracy: 0.5495 - val_loss: 0.6917 - val_accuracy: 0.5383\n",
      "Epoch 7/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6899 - accuracy: 0.5495 - val_loss: 0.6911 - val_accuracy: 0.5383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6894 - accuracy: 0.5495 - val_loss: 0.6906 - val_accuracy: 0.5383\n",
      "Epoch 9/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6889 - accuracy: 0.5567 - val_loss: 0.6906 - val_accuracy: 0.5383\n",
      "Epoch 10/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6886 - accuracy: 0.5604 - val_loss: 0.6903 - val_accuracy: 0.5692\n",
      "Epoch 11/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6884 - accuracy: 0.5713 - val_loss: 0.6898 - val_accuracy: 0.5675\n",
      "Epoch 12/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6881 - accuracy: 0.5718 - val_loss: 0.6894 - val_accuracy: 0.5783\n",
      "Epoch 13/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6879 - accuracy: 0.5737 - val_loss: 0.6893 - val_accuracy: 0.5742\n",
      "Epoch 14/200\n",
      "700/700 [==============================] - 2s 3ms/step - loss: 0.6876 - accuracy: 0.5736 - val_loss: 0.6892 - val_accuracy: 0.5808\n",
      "Epoch 15/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6876 - accuracy: 0.5726 - val_loss: 0.6885 - val_accuracy: 0.5733\n",
      "Epoch 16/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6873 - accuracy: 0.5726 - val_loss: 0.6885 - val_accuracy: 0.5792\n",
      "Epoch 17/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6872 - accuracy: 0.5761 - val_loss: 0.6883 - val_accuracy: 0.5858\n",
      "Epoch 18/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6871 - accuracy: 0.5741 - val_loss: 0.6879 - val_accuracy: 0.5704\n",
      "Epoch 19/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6869 - accuracy: 0.5775 - val_loss: 0.6882 - val_accuracy: 0.5754\n",
      "Epoch 20/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6868 - accuracy: 0.5751 - val_loss: 0.6880 - val_accuracy: 0.5763\n",
      "Epoch 21/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6869 - accuracy: 0.5768 - val_loss: 0.6881 - val_accuracy: 0.5725\n",
      "Epoch 22/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6867 - accuracy: 0.5768 - val_loss: 0.6879 - val_accuracy: 0.5783\n",
      "Epoch 23/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6864 - accuracy: 0.5763 - val_loss: 0.6887 - val_accuracy: 0.5733\n",
      "Epoch 24/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6865 - accuracy: 0.5761 - val_loss: 0.6874 - val_accuracy: 0.5792\n",
      "Epoch 25/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6863 - accuracy: 0.5744 - val_loss: 0.6873 - val_accuracy: 0.5808\n",
      "Epoch 26/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6863 - accuracy: 0.5744 - val_loss: 0.6876 - val_accuracy: 0.5813\n",
      "Epoch 27/200\n",
      "700/700 [==============================] - 2s 3ms/step - loss: 0.6862 - accuracy: 0.5788 - val_loss: 0.6870 - val_accuracy: 0.5804\n",
      "Epoch 28/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6861 - accuracy: 0.5766 - val_loss: 0.6869 - val_accuracy: 0.5821\n",
      "Epoch 29/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6861 - accuracy: 0.5779 - val_loss: 0.6868 - val_accuracy: 0.5813\n",
      "Epoch 30/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6860 - accuracy: 0.5775 - val_loss: 0.6867 - val_accuracy: 0.5767\n",
      "Epoch 31/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6859 - accuracy: 0.5766 - val_loss: 0.6870 - val_accuracy: 0.5758\n",
      "Epoch 32/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6861 - accuracy: 0.5749 - val_loss: 0.6870 - val_accuracy: 0.5742\n",
      "Epoch 33/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6858 - accuracy: 0.5770 - val_loss: 0.6867 - val_accuracy: 0.5817\n",
      "Epoch 34/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6856 - accuracy: 0.5757 - val_loss: 0.6866 - val_accuracy: 0.5821\n",
      "Epoch 35/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6858 - accuracy: 0.5757 - val_loss: 0.6863 - val_accuracy: 0.5808\n",
      "Epoch 36/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6858 - accuracy: 0.5757 - val_loss: 0.6864 - val_accuracy: 0.5796\n",
      "Epoch 37/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6855 - accuracy: 0.5752 - val_loss: 0.6866 - val_accuracy: 0.5779\n",
      "Epoch 38/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6855 - accuracy: 0.5781 - val_loss: 0.6864 - val_accuracy: 0.5792\n",
      "Epoch 39/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6854 - accuracy: 0.5799 - val_loss: 0.6862 - val_accuracy: 0.5788\n",
      "Epoch 40/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6855 - accuracy: 0.5754 - val_loss: 0.6861 - val_accuracy: 0.5829\n",
      "Epoch 41/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6853 - accuracy: 0.5767 - val_loss: 0.6870 - val_accuracy: 0.5721\n",
      "Epoch 42/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6854 - accuracy: 0.5793 - val_loss: 0.6863 - val_accuracy: 0.5800\n",
      "Epoch 43/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6853 - accuracy: 0.5771 - val_loss: 0.6857 - val_accuracy: 0.5817\n",
      "Epoch 44/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6852 - accuracy: 0.5746 - val_loss: 0.6862 - val_accuracy: 0.5800\n",
      "Epoch 45/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6853 - accuracy: 0.5772 - val_loss: 0.6863 - val_accuracy: 0.5779\n",
      "Epoch 46/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6854 - accuracy: 0.5785 - val_loss: 0.6862 - val_accuracy: 0.5804\n",
      "Epoch 47/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6853 - accuracy: 0.5779 - val_loss: 0.6857 - val_accuracy: 0.5779\n",
      "Epoch 48/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6852 - accuracy: 0.5790 - val_loss: 0.6862 - val_accuracy: 0.5804\n",
      "Epoch 49/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6850 - accuracy: 0.5788 - val_loss: 0.6861 - val_accuracy: 0.5800\n",
      "Epoch 50/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6852 - accuracy: 0.5760 - val_loss: 0.6863 - val_accuracy: 0.5808\n",
      "Epoch 51/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6847 - accuracy: 0.5780 - val_loss: 0.6862 - val_accuracy: 0.5817\n",
      "Epoch 52/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6852 - accuracy: 0.5788 - val_loss: 0.6861 - val_accuracy: 0.5779\n",
      "Epoch 53/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6849 - accuracy: 0.5759 - val_loss: 0.6853 - val_accuracy: 0.5825\n",
      "Epoch 54/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6852 - accuracy: 0.5775 - val_loss: 0.6853 - val_accuracy: 0.5863\n",
      "Epoch 55/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6849 - accuracy: 0.5771 - val_loss: 0.6855 - val_accuracy: 0.5788\n",
      "Epoch 56/200\n",
      "700/700 [==============================] - 2s 3ms/step - loss: 0.6849 - accuracy: 0.5809 - val_loss: 0.6850 - val_accuracy: 0.5867\n",
      "Epoch 57/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6849 - accuracy: 0.5768 - val_loss: 0.6852 - val_accuracy: 0.5825\n",
      "Epoch 58/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6849 - accuracy: 0.5787 - val_loss: 0.6857 - val_accuracy: 0.5792\n",
      "Epoch 59/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6850 - accuracy: 0.5787 - val_loss: 0.6859 - val_accuracy: 0.5796\n",
      "Epoch 60/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6848 - accuracy: 0.5774 - val_loss: 0.6853 - val_accuracy: 0.5771\n",
      "Epoch 61/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6848 - accuracy: 0.5761 - val_loss: 0.6858 - val_accuracy: 0.5783\n",
      "Epoch 62/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6848 - accuracy: 0.5778 - val_loss: 0.6855 - val_accuracy: 0.5783\n",
      "Epoch 63/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.6848 - accuracy: 0.5771 - val_loss: 0.6859 - val_accuracy: 0.5842\n",
      "Epoch 63: early stopping\n",
      "Validation Accuracy: 58.42%\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - 2s 3ms/step - loss: 1.3975 - accuracy: 0.5497 - val_loss: 1.1189 - val_accuracy: 0.5625\n",
      "Epoch 2/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.9479 - accuracy: 0.5596 - val_loss: 0.8260 - val_accuracy: 0.5596\n",
      "Epoch 3/200\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.7724 - accuracy: 0.5562 - val_loss: 0.7389 - val_accuracy: 0.5383\n",
      "Epoch 4/200\n",
      " 49/700 [=>............................] - ETA: 1s - loss: 0.7343 - accuracy: 0.5663"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-02f4bd41748d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     history = model.fit(x=X_train, y=Y_train, validation_data=(X_val, Y_val), \n\u001b[0m\u001b[1;32m     23\u001b[0m                         batch_size=16, epochs=epochs, callbacks=callbacks, verbose=1)\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1407\u001b[0m                 _r=1):\n\u001b[1;32m   1408\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1410\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2451\u001b[0m       (graph_function,\n\u001b[1;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2454\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1859\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1860\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1861\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    498\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Loop through and train multiple models\n",
    "epochs = 200\n",
    "learning_rate = 5e-4\n",
    "regularizer = \"l1\"\n",
    "\n",
    "models = []\n",
    "for i in range(7): # odd number for voting\n",
    "    # Make the model\n",
    "    input_layer = Input(shape=(X_train.shape[1]), name=\"wl_input\")\n",
    "    hidden_layer_1 = Dense(10, activation=\"relu\", kernel_regularizer=regularizer, name=\"wl_hl_1\")(input_layer)\n",
    "    hidden_layer_2 = Dense(5, activation=\"relu\", kernel_regularizer=regularizer, name=\"wl_hl_2\")(hidden_layer_1)\n",
    "    output_layer = Dense(1, activation=\"sigmoid\", name=\"wl_output\")(hidden_layer_2)\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    \n",
    "    # Compile and train the model\n",
    "    loss = tf.keras.losses.BinaryCrossentropy()\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    metrics = ['accuracy']\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "    \n",
    "    callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=7, verbose=1)]\n",
    "    history = model.fit(x=X_train, y=Y_train, validation_data=(X_val, Y_val), \n",
    "                        batch_size=16, epochs=epochs, callbacks=callbacks, verbose=1)\n",
    "    \n",
    "    # Print final model accuracy\n",
    "    val_acc = model.evaluate(X_val, Y_val, return_dict=True, verbose=0)['accuracy']\n",
    "    print(f\"Validation Accuracy: {val_acc*100:.2f}%\")\n",
    "    if val_acc < 0.55:\n",
    "        print(\"Skipped.\")\n",
    "        continue\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92be1488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the model architecture\n",
    "tf.keras.utils.plot_model(models[0], to_file='model_images/wl_model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55464e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group predict\n",
    "all_preds = np.array([m.predict(X_val, verbose=0).reshape(-1) for m in models])\n",
    "\n",
    "mean_preds = all_preds.mean(axis=0) > 0.5\n",
    "mean_acc = np.mean(mean_preds == Y_val)\n",
    "\n",
    "vote_preds = (all_preds > 0.5).sum(axis=0) > (all_preds.shape[0]/2)\n",
    "vote_acc = np.mean(vote_preds == Y_val)\n",
    "\n",
    "print(f\"Mean Accuracy: {mean_acc*100:.2f}%\")\n",
    "print(f\"Vote Accuracy: {vote_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84359e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group testing\n",
    "all_preds = np.array([m.predict(X_test, verbose=0).reshape(-1) for m in models])\n",
    "\n",
    "mean_preds = all_preds.mean(axis=0) > 0.5\n",
    "mean_acc = np.mean(mean_preds == Y_test)\n",
    "\n",
    "vote_preds = (all_preds > 0.5).sum(axis=0) > (all_preds.shape[0]/2)\n",
    "vote_acc = np.mean(vote_preds == Y_test)\n",
    "\n",
    "print(f\"Mean Accuracy: {mean_acc*100:.2f}%\")\n",
    "print(f\"Vote Accuracy: {vote_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046fd1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, model in enumerate(models):\n",
    "    model.save(f\"saved_models/win_loss_model/model_{i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03744431",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
